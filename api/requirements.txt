fastapi>=0.100.0
uvicorn[standard]>=0.22.0
pydantic>=2.0
pydantic-settings>=2.0
torch>=2.0.0
transformers>=4.36.0  # Updated for better Flash Attention 2 support
python-multipart
pytest
httpx
pyyaml
peft>=0.6.0
numpy>=1.24.0

# Optional: Flash Attention 2 (requires CUDA, install separately)
# pip install flash-attn --no-build-isolation
# flash-attn>=2.0.0
