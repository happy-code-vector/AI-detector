# Training configuration for AI Detector
model: microsoft/deberta-v3-base
epochs: 3
batch_size: 16
learning_rate: 2e-5
max_length: 128
warmup_steps: 500
output_dir: ../api/models/checkpoint-best
logging_steps: 50
eval_steps: 500
save_steps: 500

# Quantization & PEFT settings (for efficient training)
# Options: none, 8bit, 4bit
load_in_8bit: false
load_in_4bit: false

# PEFT/LoRA settings
use_peft: false
lora_r: 16          # LoRA attention dimension
lora_alpha: 32      # LoRA scaling parameter
lora_dropout: 0.1   # LoRA dropout

# Data settings
max_sentences: 120
max_words_per_sentence: 50
train_split: 0.8
eval_split: 0.1
test_split: 0.1

# Public datasets (Hugging Face)
# public_datasets:
#   - fake-news
#   - ai-generated-text-dataset

# Custom dataset paths
custom_data_path: data/custom/sample.json
