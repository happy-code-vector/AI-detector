# LoRA-only training configuration (no quantization)
# Works on Windows and trains efficiently with LoRA adapters

model: microsoft/deberta-v3-base
epochs: 3
batch_size: 16
learning_rate: 1e-5  # Lowered for stability
max_length: 128
warmup_steps: 500
output_dir: ../api/models/checkpoint-best
logging_steps: 50
eval_steps: 500
save_steps: 500

# Gradient clipping to prevent training divergence
max_grad_norm: 1.0

# No quantization
load_in_8bit: false
load_in_4bit: false

# PEFT/LoRA settings (reduces trainable params to ~1%)
use_peft: true
lora_r: 16
lora_alpha: 32
lora_dropout: 0.1

# Data settings
max_sentences: 120
max_words_per_sentence: 50
train_split: 0.8
eval_split: 0.1
test_split: 0.1

# Custom dataset path
custom_data_path: data/custom/ai_polished_full.json
