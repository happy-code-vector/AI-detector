# 4-bit QLoRA training configuration
# Maximum efficiency with PEFT/LoRA

model: microsoft/deberta-v3-base
epochs: 3
batch_size: 16
learning_rate: 2e-5
max_length: 128
warmup_steps: 500
output_dir: ../api/models/checkpoint-best
logging_steps: 50
eval_steps: 500
save_steps: 500

# Enable 4-bit quantization with LoRA (~75% memory savings, 1-3% accuracy loss)
load_in_8bit: false
load_in_4bit: true

# PEFT/LoRA settings
use_peft: true
lora_r: 16          # LoRA attention dimension
lora_alpha: 32      # LoRA scaling parameter
lora_dropout: 0.1   # LoRA dropout

# Data settings
max_sentences: 120
max_words_per_sentence: 50
train_split: 0.8
eval_split: 0.1
test_split: 0.1

# Custom dataset path
custom_data_path: data/custom/ai_polished_full.json
