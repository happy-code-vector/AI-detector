# Core dependencies
torch>=2.0.0
transformers>=4.36.0  # Updated for better Flash Attention 2 support
datasets>=2.12.0
accelerate>=0.20.0

# Training utilities
scikit-learn
pandas
tqdm
numpy
pyyaml

# PEFT/LoRA for efficient training
peft>=0.18.0

# Tokenization
sentencepiece
protobuf

# Optional: Flash Attention 2 for 2-4x speedup on Ampere+ GPUs
# (RTX 3090/4090, A100, H100, H200, B200, A40, A5000, A6000)
# NOT supported on: T4, RTX 3060, A4000, A10G (uses eager fallback)
# Install with: pip install flash-attn --no-build-isolation
flash-attn>=2.0.0

# Optional: experiment tracking
# wandb

# Additional utilities
colorama
