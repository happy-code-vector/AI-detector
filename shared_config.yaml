# Shared configuration for AI Detector project
# This is the single source of truth for model and data parameters
# Both training and API read from this file
#
# GPU AUTO-CONFIGURATION:
# Training automatically detects GPU and applies optimal settings.
# To disable: python train.py --no-auto-gpu
# To manually set: python train.py --gpu "H100"
# To list supported: python train.py --list-gpus
#
# GPU PRESETS (auto-applied):
# ┌──────────┬───────┬─────────┬────────────────┬─────────────────────┐
# │ GPU      │ VRAM  │ Batch   │ Grad Acc       │ Effective Batch     │
# ├──────────┼───────┼─────────┼────────────────┼─────────────────────┤
# │ RTX 3060 │ 12GB  │ 2       │ 8              │ 16                  │
# │ RTX 3090 │ 24GB  │ 8       │ 2              │ 16                  │
# │ RTX 4090 │ 24GB  │ 8       │ 2              │ 16                  │
# │ H100     │ 80GB  │ 16      │ 1              │ 16                  │
# │ H200     │ 141GB │ 24      │ 1              │ 24                  │
# │ B200     │ 192GB │ 32      │ 1              │ 32                  │
# └──────────┴───────┴─────────┴────────────────┴─────────────────────┘

model:
  # Base model name from HuggingFace
  name: "microsoft/deberta-v3-large"

  # Where trained models are saved/loaded from
  checkpoint_dir: "api/models/checkpoint-best"

  # Device: auto, cuda, cpu
  device: "auto"

data:
  # Full dataset path (relative to project root)
  full_data_path: "training/data/custom/all_datasets_combined.json"

  # Test dataset path (subset for local testing)
  test_data_path: "training/data/custom/test_subset.json"

  # Text processing limits
  max_sentences: 120
  max_words_per_sentence: 350

  # Data split ratios
  train_split: 0.8
  eval_split: 0.1
  test_split: 0.1

  # Test mode: use small subset for quick testing
  test_subset_size: 5000

training:
  # Training mode: "test" or "full"
  mode: "full"

  # NOTE: batch_size and gradient_accumulation_steps are AUTO-CONFIGURED
  # based on detected GPU. Values below are fallback defaults.
  # Use --no-auto-gpu to disable auto-configuration.
  batch_size: 4
  gradient_accumulation_steps: 4

  # Learning rate
  learning_rate: 2e-5

  # Epochs
  epochs: 3

  # Max sequence length
  max_length: 512

  # Gradient clipping
  max_grad_norm: 1.0

  # Warmup: use ratio (scales with dataset size)
  warmup_ratio: 0.05

  # Weight decay for regularization
  weight_decay: 0.01

  # Logging/evaluation steps
  logging_steps: 50
  eval_steps: 500
  save_steps: 500

  # LoRA settings for efficient fine-tuning
  use_peft: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1

api:
  # API server batch size
  batch_size: 8

  # Inference optimization
  inference_batch_size: 32
  max_batch_tokens: 8192

  # Server settings
  host: "0.0.0.0"
  port: 8000
  reload: true

  # API metadata
  title: "AI Text Detector API"
  version: "1.0.0"
  description: "Word-level AI-generated text detection API"
