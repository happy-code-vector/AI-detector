# Shared configuration for AI Detector project
# This is the single source of truth for model and data parameters
# Both training and API read from this file

model:
  # Base model name from HuggingFace
  name: "microsoft/deberta-v3-large"

  # Where trained models are saved/loaded from
  # Relative to project root
  # To use ONNX (recommended for CPU): change to "api/models/model_onnx"
  checkpoint_dir: "api/models/checkpoint-best"

  # Device: auto, cuda, cpu
  device: "auto"

  # CPU optimization (set number of threads for ONNX Runtime)
  # Set to number of physical CPU cores for optimal performance
  omp_num_threads: 12

data:
  # Custom dataset path (relative to project root)
  custom_data_path: "training/data/custom/all_datasets_combined.json"

  # Text processing limits
  max_sentences: 120
  max_words_per_sentence: 350  # Updated policy: 7x increase from 50

  # Data split ratios
  train_split: 0.8
  eval_split: 0.1
  test_split: 0.1

training:
  # batch_size: 12 = effective batch size of 12 with no accumulation
  # Adjust down if OOM: 8 or 10
  batch_size: 4

  # Gradient accumulation (reduced since we have larger batch size)
  # effective_batch_size = 12 * 2 = 24
  gradient_accumulation_steps: 4

  # Learning rate
  learning_rate: 2e-5  # Slightly higher for faster convergence

  # Epochs
  epochs: 3

  # Max sequence length
  # Increased to 512 to support 350 words per sentence (~455 tokens at 1.3 tokens/word)
  max_length: 512

  # Gradient clipping
  max_grad_norm: 1.0

  # Warmup steps
  warmup_steps: 500

  # Logging/evaluation steps
  logging_steps: 50
  eval_steps: 500
  save_steps: 500

  # Quantization for memory efficiency
  # 8-bit/4-bit can cause PEFT/LoRA compatibility issues
  load_in_8bit: true
  load_in_4bit: false

  # LoRA settings
  use_peft: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1

api:
  # API server batch size (can be smaller for real-time inference)
  batch_size: 8

  # Inference optimization
  # Larger batch_size = faster throughput but more memory
  inference_batch_size: 32
  use_fp16: true  # Half precision for 2x speed (CUDA only)
  max_batch_tokens: 8192  # Max tokens per batch (prevents OOM)

  # Server settings
  host: "0.0.0.0"
  port: 8000
  reload: true

  # API metadata
  title: "AI Text Detector API"
  version: "1.0.0"
  description: "Word-level AI-generated text detection API"
