# Shared configuration for AI Detector project
# This is the single source of truth for model and data parameters
# Both training and API read from this file

model:
  # Base model name from HuggingFace
  name: "microsoft/deberta-v3-large"

  # Where trained models are saved/loaded from
  # Relative to project root
  checkpoint_dir: "api/models/checkpoint-best"

  # Device: auto, cuda, cpu
  device: "auto"

data:
  # Custom dataset path (relative to project root)
  custom_data_path: "training/data/custom/ai_polished_full.json"

  # Text processing limits
  max_sentences: 120
  max_words_per_sentence: 50

  # Data split ratios
  train_split: 0.8
  eval_split: 0.1
  test_split: 0.1

training:
  # Default training batch size
  batch_size: 16

  # Learning rate
  learning_rate: 1e-5

  # Epochs
  epochs: 3

  # Max sequence length
  max_length: 128

  # Gradient clipping
  max_grad_norm: 1.0

  # Warmup steps
  warmup_steps: 500

  # Logging/evaluation steps
  logging_steps: 50
  eval_steps: 500
  save_steps: 500

  # Quantization (not recommended - compatibility issues)
  load_in_8bit: false
  load_in_4bit: false

  # LoRA settings
  use_peft: true
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1

api:
  # API server batch size (can be smaller for real-time inference)
  batch_size: 8

  # Inference optimization
  # Larger batch_size = faster throughput but more memory
  inference_batch_size: 32
  use_fp16: true  # Half precision for 2x speed (CUDA only)
  max_batch_tokens: 8192  # Max tokens per batch (prevents OOM)

  # Server settings
  host: "0.0.0.0"
  port: 8000
  reload: true

  # API metadata
  title: "AI Text Detector API"
  version: "1.0.0"
  description: "Word-level AI-generated text detection API"
